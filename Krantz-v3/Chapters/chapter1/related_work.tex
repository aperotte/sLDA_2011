This chapter covers classifying or, equivalently, labeling bag-of-words data.  We show how to construct classifiers based on features derived from mixed membership models.  We specifically develop a solution to the concrete problem of hierarchically classifying text documents.  The model and techniques developed in this chapter can be used to solve any labeling or hierarchical classification problem involving data represented as bags-of-words.  If, for instance,  an image is represented by counts of features then it is said to be in bag-of-words representation.   So, for instance, hierarchical classification of images might be tackled using the techniques developed in this chapter, although, such a particular application will not be explicitly considered.

We should agree that classification and regression are largely the same.  Namely, classification is regression with discrete output.  To solve classification problems we must learn from training instances, as in regression, a function $f : \mathcal{X} \rightarrow \mathcal{L}$ which maps from an input space $\mathcal{X}$ to an output (label) space $\mathcal{L}$.   As an example consider the problem of identifying the authors of a set of documents for which the true author is unknown but is one of a set of possible authors.   For such a problem the input space is the richly structured space of possible texts and the output space is the large but largely unstructured list of possible authors.  Training data would consist of text author pairs for texts known to have been authored by authors in the list of possible authors.

To solve such a problem one would need to choose a set of features to extract from the underlying documents. Then, using those features and the training data, a classification function would be learned.  In this case one might choose a multi-class logistic or probit regression function and learn parameters for it from the labeled training instances.   In this chapter we will focus on models that use mixed-membership representations of bag-of-word data as the input features to joint models that can be used, via conditioning, for classification or regression.

The models that we propose in this chapter generalize the author classification example in that the label spaces we will consider are also richly structured (in addition to the input space) and the regression function we learn maps from from the input space to labels in a structured, typically hierarchical label space.  In the same way that mixed membership models are surprisingly effective, the resulting hierarchical classification models seem to be too.

\subsection{Background}

Mixed membership models, particularly latent Dirichlet allocation (LDA) \cite{Blei2003}, have been reviewed in other chapters.   The key property we exploit for purposes of classification is that LDA % is a generative
%probabilistic model of corpora consisting of documents that are bag-of-words.  In particular LDA 
provides a way to extract a latent, low-dimensional representation of each bag-of-words observation consisting of mixing proportions over topic vectors.  Each topic is a frequency histogram of terms.  Each document is a mixture of topics.  This mixture of topics vector is the feature vector we exploit for purposes of document classification.  

%Also known as topic models, these models
%infer the latent structure, or topics, of documents in a corpus. Each
%document is represented as a collection of words, generated from a
%set of topic assignments (one for each word), where each topic assignment
%is drawn from a distribution over topics \cite{Blei2003}.

The idea for using this representation for classification was first considered in a paper on so-calling ``supervised'' latent Dirichlet allocation (SLDA).  SLDA built on LDA by incorporating
``supervision'' in the form of an observed exponential family response 
variable per document.  Another way of saying this is that SLDA was the first document classifier for documents in the bag-of-words representation that exploited the LDA topic feature representation.  
%A subtle point should be noted here. 
 SLDA and the subject of this chapter, a generalization of it called hierarchically supervised LDA (HSLDA) \cite{Perotte-2012-NIPS} are not, strictly speaking, regression models.  This is because they model the {\em joint} distribution of labels and bag-of-words data rather than the conditional distribution of the labels given the input.   Having the joint distribution allows us to condition on an input to produce a label for all test instances.  Additionally, during training of the model, knowing the labels effects the learning of the low dimensional representation of the input -- a characteristic discussed later in this chapter.
% This is quite different (and better performing in practice) than LASSO
%(L1 regularized least squares regression) and LDA followed by least
%squares regression \cite{BleiMcAuliffe2008}.
%By performing joint inference, SLDA and HSLDA
%infer topics such that the model predicts the response variable while
%improving word likelihood.  

%In this chapter we extend supervised latent Dirichlet allocation (SLDA)
%\cite{BleiMcAuliffe2008} to take advantage of hierarchical supervision. SLDA
%is latent Dirichlet allocation (LDA) \cite{Blei2003} augmented with per
%document ``supervision,'' often taking the form of a single numerical or
%categorical label. It has been demonstrated that the signal provided by such
%supervision can result in better, task-specific document models and can also
%lead to good label prediction for out-of-sample data \cite{BleiMcAuliffe2008}.

%It also has been demonstrated that SLDA has been shown to outperform both LASSO
%(L1 regularized least squares regression) and LDA followed by least squares
%regression~\cite{BleiMcAuliffe2008}. 

The problem of applying structured labels to bag-of-words data can be tackled in a number of different ways.  For instance,  SLDA can be used to solve this kind of problem directly; however, doing so requires ignoring the hierarchical
dependencies amongst the labels. In Section~\ref{sec:experiments} we contrast
HSLDA with SLDA applied in this way. 
Other models that incorporate LDA and supervision and so could be used to solve this problem include
LabeledLDA~\cite{Ramage2009} and DiscLDA~\cite{DiscLDA}.  Various applications of these models to 
computer vision and document networks have been explored~\cite{wangbleifeifei08,RelationalLDA}.
None of these models, however, leverage dependency structure in the label space.
%While there has been much work in multi-label classification of text and
%text modeling in general, we focus here on topic modeling approaches.
%Latent Dirichlet allocation (LDA) is a generative probabilistic model which
%represents documents as a mixed-membership bag of word. Each document is
%represented as a collection of words, generated from a set of topic assignments
%(one for each word), where each topic assignment is drawn from a distribution
%over topics~\cite{Blei2003}. SLDA is latent Dirichlet allocation (LDA)
%\cite{Blei2003} augmented with per-document labeling, often taking the form of
%a single numerical or categorical label. Examples of labels include ratings
%associated with online reviews, grades for essays, and the number of times a
%webpage is linked. This approach has been shown to outperform both LASSO
%(L1 regularized least squares regression) and LDA followed by least
%squares regression~\cite{BleiMcAuliffe2008}.





In other non-LDA-based related work, researchers have classified documents into a hierarchy (a closely related task) using naive Bayes classifiers
and support vector machines. Most of this work has been demonstrated on relatively
small datasets, small label spaces, and has focused on single label classification without
a model of documents such as LDA~\cite{mccallum99building,Dumais2000HCW,Kollerilprints291,Chakrabarti1998SFS}.  The HSLDA model we cover here is not limited in this way.

%One set of models that are particularly relevant to HSLDA are Chang
%and Blei's hierarchical models for document networks (Relational Topic
%Models). In that family of models, they encountered a similar scenario
%where the lack of a link did not truly indicate absence. In hierarchically
%labeled data, negative labels are uncommon and the lack of a label
%in the hierarchy is not equivalent to a negative label. Therefore,
%as in the work of Chang and Blei, we employ regularization to account
%for the lack of negative labels. This will be discussed further in

% Noemie: check out what I wrote here, because I am not really sure this is
% correct. 
%There have been several models that incorporate both latent models of text and
%some form of
%supervision~\cite{Ramage2009,DiscLDA,wangbleifeifei08,RelationalLDA}. One set 
%of models that are particularly relevant to HSLDA are Chang and Blei's
%hierarchical models for document networks (Relational Topic Models). In that
%family of models, they encountered a similar scenario where an unselected label 
%does not always indicate absence. In hierarchical labels, this phenomenon is
%even more pervasive -- there are no explicit negative labels, but it is also
%unclear how to treat the parents of selected labels. Like in the work of Chang
%and Blei, we employ regularization to account for the lack of negative
%labeling. In our experiments, we look at the impact of assigning positive and
%negative instances to the ancestors of selected labels.
