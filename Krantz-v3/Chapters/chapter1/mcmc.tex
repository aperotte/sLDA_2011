Markov chain Monte Carlo (MCMC) is the name given to a particular technique for generating samples from distributions of interest.  In Bayesian analyses this distribution is often the posterior distribution of the latent quantities given the data and fixed parameters.  The technique is general, elegant, and simple.  The basic idea is that by using Markov chain transition kernel templates, it is possible to construct and simulate a Markov chain whose equilibrium distribution is the distribution of interest.   This means that  the sequence of Markov chain simulation outputs can be used as an empirical stand-in approximation to the distribution of interest.  What is more, general MCMC techniques don't rely upon having a normalized probability distribution as a starting point, merely a function that is at all points proportional to the distribution of interest.   This is particularly convenient in the Bayesian case where the posterior distribution is of interest and does not have an analytic form.  In this case, the posterior distribution is simply proportion to the joint distribution with the observed quantities fixed to their observed values.

Let $\mathbf{z}$ be a set of latent variables.  Let $\mathbf{x}$ be a set of observed quantities.  We could denote by $\theta$ any set of fixed parameters and include it on the conditioning side of every expression to follow in this section but, since it would appear in every expression we'll omit it for purposes of notational clarity.  Our goal is to generate samples distributed according to $P(\mathbf{z} | \mathbf{x})$ (one can think about sampling from a posterior distribution here, however, MCMC applies more generally).  We have a model whose joint distribution is specified and can be evaluated readily for all settings of $\mathbf{z}$ and $\mathbf{x}$.  By Bayes rule we know that

\[P(\mathbf{z} | \mathbf{x} = (x_1, \ldots, x_N) ) \propto P(\mathbf{z}, \mathbf{x}= (x_1, \ldots, x_N)).\]

We denote by $\tilde{P}(\mathbf{z} | \mathbf{x} = (x_1, \ldots, x_N) )$ the normalized version of $P(\mathbf{z} | \mathbf{x} = (x_1, \ldots, x_N) )$.  Assume that we can come up with a function $P(\mathbf{z}^* | \mathbf{z})$ that has the following property (called )

\[P(\mathbf{z*} | \mathbf{x} = (x_1, \ldots, x_N) ) = \sum_ \mathbf{z} P(\mathbf{z}^* | \mathbf{z}) P(\mathbf{z} | \mathbf{x} = (x_1, \ldots, x_N) )\]