% !TEX root =  ../../Krantz_style.tex

Documents frequently come with additional information like labels or popularity ratings.   Contemporary examples include the product ratings that  accompany product descriptions, the number of ``likes'' that webpages have attracted, grades associated with assigned essays, and so forth.

This chapter covers one way to jointly model documents and the labels applied to them.  In particular we focus on our own work on modeling documents with more complicated labels that themselves possess some kind of structural organization.  Consider typical product catalogs.  They usually contain text  descriptions of products that have been organized into hierarchical product directories.  The situation of a product into such a hierarchy (the path or paths in the product hierarchy that lead to it) can be thought of as a structured label.  Jointly modeling the document and such a label is useful for automatically labeling new documents (corresponding in this example to automatically situating a new product in the product directory) and more.

Collections of hierarchically labelled  documents abound, text and otherwise.  We will consider hierarchically labelled  patient clinical records in later sections.  Applications like situating web-pages in hierarchical link directories are left for others to explore.

%
%This chapter covers classifying and labeling bag-of-words data.  %We show how to construct classifiers based on features derived from mixed membership models.  
%We specifically develop a solution to the concrete problem of hierarchically classifying text documents.  The model and techniques developed in this chapter can be used to solve any labeling or hierarchical classification problem involving data represented as bags-of-words.  For instance,  if an image is represented by counts of features then it can be described as being in a bag-of-words representation.   So, hierarchical classification of images might be tackled using the techniques developed in this chapter, although, such a particular application will not be explicitly considered.
%
%
%
%
%%We should agree that classification and regression are largely the same.  Namely, classification is regression with discrete output.  
%To solve classification problems we must learn from training instances, as in regression, a function $f : \mathcal{X} \rightarrow \mathcal{L}$ which maps from an input space $\mathcal{X}$ to an output (label) space $\mathcal{L}$.   For example, consider the problem of identifying the authors of a set of documents for which the true author is unknown but is one of a set of possible authors.   For such a problem the input space is the richly structured space of possible texts and the output space is the large but largely unstructured list of possible authors.  Training data would consist of text/author pairs for texts known to have been authored by authors in the list of possible authors.
%
Because text documents are notorious difficult to directly model we take an approach common to other chapters in this book.  We use a mixed-membership model of the text document to represent the document as bag-of-words drawn from a document specific mixture of topic distributions.   The modeling choices we make in relating this representation to structured labels follows, as too its relationship to prior art.

%solve our problem one would need to choose a set of features to extract from the underlying documents.  In this chapter we will focus on models that use mixed-membership representations of bag-of-word data as the input features to joint models that can be used, via conditioning, for classification or regression.

%The models that we propose in this chapter generalize the author classification example in that the label spaces we will consider are also richly structured (in addition to the input space) and the regression function we learn maps from the input space to labels in a structured, typically hierarchical label space.  In the same way that mixed membership models are surprisingly effective, the resulting hierarchical classification models seem to be too.

\subsection{Background}

Mixed membership models, including the model upon which we build, latent Dirichlet allocation (LDA) \cite{Blei2003}, have been reviewed in other chapters.   The key property we exploit for purposes of classification is that LDA % is a generative
%probabilistic model of corpora consisting of documents that are bag-of-words.  In particular LDA 
provides a way to extract a latent, low-dimensional representation of text and other documents consisting of the  frequency of word assignments to the topics that are assumed to have generated them.  A topic is a distribution over words.  Each document is a bag of words drawn from a document specific mixture of topics.  %This mixture of topics vector is the feature vector we exploit for purposes of document classification.  

%Also known as topic models, these models
%infer the latent structure, or topics, of documents in a corpus. Each
%document is represented as a collection of words, generated from a
%set of topic assignments (one for each word), where each topic assignment
%is drawn from a distribution over topics \cite{Blei2003}.

Building a joint model of documents and labels using this representation is not new.  It was first introduced by Blei and McAuliffe in a paper on 
 ``supervised'' latent Dirichlet allocation (SLDA) \cite{BleiMcAuliffe2008}.  SLDA built on LDA by incorporating
``supervision'' in the form of an observed exponential family response 
variable per document.  

\subsubsection{Latent Dirichlet Allocation}

To explain both SLDA and to set the stage for our work it helps to introduce our notation for LDA.  Assume that there are $K$ topics.  Let $\boldsymbol\phi_{k}\sim{\rm Dir}_{V}(\gamma\mathbf{1}_V)$ be ``topic''  $k$, i.e.~a distribution over a finite set of words.  Here $V$ simply labels the variables to indicate that they have to do with the vocabulary.  The vector $\mathbf{1}_V$ consists of all ones and has length equal to the size of the vocabulary.  The distribution ${\rm Dir}$ is the Dirichlet distribution.  The constant $\gamma$ controls the smoothness of the inferred topics.  Larger values lead to smoother topic estimates.  Let $\boldsymbol\beta\mid\alpha'\sim{\rm Dir}_{K}\left(\alpha^{\prime}\mathbf{1}_K\right)$  be a ``global'' distribution over topics where $K$ indicates that the distribution and variable sizes are equal to the number of topics $K$ and $\alpha^{\prime}$ controls the relative proportion of topics globally, large $\alpha^\prime$ leading to all topics being roughly responsible for the same number of words.  Intuitively $\boldsymbol\beta$ is something like the average topic proportion independent of any particular document.  Per document $d$ topic distributions $\boldsymbol\theta_d\mid\boldsymbol\beta,\alpha\sim{\rm Dir}_{K}\left(\alpha\boldsymbol\beta\right)$ are modeled as being deviations from the global distribution over topics where larger values of $\alpha$ result in all documents' topic distributions being more similar.  We will use  $z_{n,d}\mid\boldsymbol\theta_d\sim{\rm Multinomial}(\boldsymbol\theta_d)$ to indicate which topic generated the $n$th word of document $d$.   Drawing $n$th word in document $d$ from the  indicated topic $w_{n,d}\mid z_{n,d},\boldsymbol\phi_{1:K}\sim{\rm Multinomial}(\boldsymbol\phi_{z_{n,d}})$ completes our notation of standard LDA.    

\subsubsection{Supervised Latent Dirichlet Allocation}


Supervised LDA adds another per document observation, a label $y_d$.  It is modeled as being generated by a generalized linear model (GLM) $y_d | \bar{{\mathbf z}}_d, \boldsymbol\eta,\delta \sim \mathrm{GLM}(\bar{\mathbf{z}}_d,\boldsymbol\eta,\delta)$.  Brushing aside exponential family and GLM link function generalities, what SLDA does is regress labels against the empirical distributions of assigned topic indicator variables $\bar{\mathbf{z}}_d = \{\bar{z}_{1,d}, \ldots, \bar{z}_{K,d}\}$ where $\bar{z}_{k,d} = \frac{\sum_n \mathbb{I}(z_{n,d}=k)}{\sum_n \sum_j \mathbb{I}(z_{n,d}=j)}$ is the fraction of words assigned to topic $k$ ($\mathbb{I}(\cdot)$ is the indicator function that returns one if its argument is true).
If the document labels are real valued then one example choice for the regression relationship would be $y_d | {\bar{\mathbf z}}_d, \boldsymbol\eta \sim \mathcal{N}(\bar{\mathbf{z}}_d^T\boldsymbol\eta,\delta).$  Using a generalized linear model in the exponential family to parameterize this regression relationship allows for a wide variety of distributions over different kinds of label spaces to be represented in the same mathematical formalism.   A variational expectation maximization algorithm was proposed in \cite{BleiMcAuliffe2008} to learn model parameters.  Experimental results in \cite{BleiMcAuliffe2008}  showed both excellent out of sample label prediction and improved topics.  Topic improvement was measured by using the empirical topic proportions from SLDA  as features for external, discriminative approaches to label prediction.  Regression models built on SLDA topics outperformed the same built on LDA derived topics.

In one sense SLDA is more general than the model we present in this chapter, namely, the labels need not be categorical valued.  %In our $y_d | {\bar{\mathbf z}}_d, \boldsymbol\eta \sim \mathcal{N}(\bar{\mathbf{z}}_d^T\boldsymbol\eta,\delta)$ example the labels are real-valued for instance.  
The main subject of this chapter, a generalization of SLDA called hierarchically supervised LDA (HSLDA) \cite{Perotte-2012-NIPS} does not deal with real-valued labels, however, it is more general than SLDA in the case of categorical labels.  The exponential family/GLM regression framework can theoretically account for multivariate labels and potentially even structured categorical labels.   HSLDA is, however,  a specific, practical way to model with structured categorical labels.  Because we focus on hierarchically structured categorical labels we refer to our model as a mixed membership hierarchical classification model. 

%
%Another way of saying this is that SLDA was the first document classifier for documents in the bag-of-words representation that exploited the LDA topic feature representation.  
%%A subtle point should be noted here. 
% SLDA and the subject of this chapter, a generalization of it called hierarchically supervised LDA (HSLDA) \cite{Perotte-2012-NIPS} are not, strictly speaking, regression models.  This is because they model the {\em joint} distribution of labels and bag-of-words data rather than the conditional distribution of the labels given the input.   Having the joint distribution allows us to condition on an input to produce a label for all test instances.  Additionally, during training of the model, knowing the labels effects the learning of the low dimensional representation of the input -- a characteristic discussed later in this chapter.
% This is quite different (and better performing in practice) than LASSO
%(L1 regularized least squares regression) and LDA followed by least
%squares regression \cite{BleiMcAuliffe2008}.
%By performing joint inference, SLDA and HSLDA
%infer topics such that the model predicts the response variable while
%improving word likelihood.  

%In this chapter we extend supervised latent Dirichlet allocation (SLDA)
%\cite{BleiMcAuliffe2008} to take advantage of hierarchical supervision. SLDA
%is latent Dirichlet allocation (LDA) \cite{Blei2003} augmented with per
%document ``supervision,'' often taking the form of a single numerical or
%categorical label. It has been demonstrated that the signal provided by such
%supervision can result in better, task-specific document models and can also
%lead to good label prediction for out-of-sample data \cite{BleiMcAuliffe2008}.

%It also has been demonstrated that SLDA has been shown to outperform both LASSO
%(L1 regularized least squares regression) and LDA followed by least squares
%regression~\cite{BleiMcAuliffe2008}. 


%While there has been much work in multi-label classification of text and
%text modeling in general, we focus here on topic modeling approaches.
%Latent Dirichlet allocation (LDA) is a generative probabilistic model which
%represents documents as a mixed-membership bag of word. Each document is
%represented as a collection of words, generated from a set of topic assignments
%(one for each word), where each topic assignment is drawn from a distribution
%over topics~\cite{Blei2003}. SLDA is latent Dirichlet allocation (LDA)
%\cite{Blei2003} augmented with per-document labeling, often taking the form of
%a single numerical or categorical label. Examples of labels include ratings
%associated with online reviews, grades for essays, and the number of times a
%webpage is linked. This approach has been shown to outperform both LASSO
%(L1 regularized least squares regression) and LDA followed by least
%squares regression~\cite{BleiMcAuliffe2008}.







%One set of models that are particularly relevant to HSLDA are Chang
%and Blei's hierarchical models for document networks (Relational Topic
%Models). In that family of models, they encountered a similar scenario
%where the lack of a link did not truly indicate absence. In hierarchically
%labeled data, negative labels are uncommon and the lack of a label
%in the hierarchy is not equivalent to a negative label. Therefore,
%as in the work of Chang and Blei, we employ regularization to account
%for the lack of negative labels. This will be discussed further in

% Noemie: check out what I wrote here, because I am not really sure this is
% correct. 
%There have been several models that incorporate both latent models of text and
%some form of
%supervision~\cite{Ramage2009,DiscLDA,wangbleifeifei08,RelationalLDA}. One set 
%of models that are particularly relevant to HSLDA are Chang and Blei's
%hierarchical models for document networks (Relational Topic Models). In that
%family of models, they encountered a similar scenario where an unselected label 
%does not always indicate absence. In hierarchical labels, this phenomenon is
%even more pervasive -- there are no explicit negative labels, but it is also
%unclear how to treat the parents of selected labels. Like in the work of Chang
%and Blei, we employ regularization to account for the lack of negative
%labeling. In our experiments, we look at the impact of assigning positive and
%negative instances to the ancestors of selected labels.
