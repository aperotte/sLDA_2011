
%\begin{itemize}
%\item Describe Data for both Experiments
%\item Describe the Four different conditions
%\item Describe how they are all evaluated, with and without hierarchy
%\end{itemize}

We experimented with HSLDA for prediction in two domains: predicting medical
diagnosis codes from hospital discharge summaries and predicting product
categories from Amazon.com product descriptions.

%In the section we describe the application of HSLDA for prediction
%in two hierarchically structured domains. Firstly, we describe using
%discharge summaries to predict diagnoses, encoded as ICD-9 codes.
%Discharge summaries are documents that are authored by clinicians
%to summarize the course of a hospitalization. ICD-9 codes are used
%mainly for billing purposes to indicate the conditions for which a
%patient was treated. Secondly, we describe using Amazon.com product
%descriptions to predict product categories.

\subsection{Data and Pre-Processing}

\subsubsection{Diagnosis Prediction}

Discharge summaries are authored by clinicians to summarize the course of a
hospitalized patient. The summaries typically contain a record of the patient
complains, findings and diagnoses, along with treatment and hospital course.
For each admission trained medical coders review the information in the
discharge summary and assign a series of diagnoses codes. Coding follows the
ICD-9-CM controlled terminology, an international diagnostic classification for
epidemiological, health management, and clinical
purposes.\footnote{http://www.cdc.gov/nchs/icd/icd9cm.htm} As such, the ICD-9
codes constitute a labeling of a patient's diagnoses based on a discharge
summary. The ICD-9 codes are organized in a rooted-tree structure, with each
edge representing an is-a relationship between parent and child, such that the
parent diagnosis subsumes the child diagnosis. For example, the code for
{}``Pneumonia due to adenovirus'' is a child of the code for {}``Viral
pneumonia,'' where the former is a type of the latter.  It is worth noting that
the coding can be noisy. Human coders sometimes disagree~\cite{Challenge07},
tend to be more specific than sensitive in their
assignments~\cite{Birmetal2005}, and sometimes make
mistakes~\cite{FarzandipourEtAl10}. 

The task of automatic ICD-9 coding has been investigated in the clinical
domain. Much of the work was triggered by the 2007 medical NLP community
challenge~\cite{Challenge07}. The data in the challenge, however, differs from 
ours in its scope. The datasets were smaller (1,000 training and 1,000 testing
documents) and focused on radiology reports with a restricted number of ICD-9
codes (45 of them, compared to 7K+ in our dataset). Methods ranged from manual
rules to online learning~\cite{Crammer2007,Goldstein2007,Farkas2008}.
Other work had leveraged larger datasets and experimented with K-nearest
neighbor, Naive Bayes, support vector machines, Bayesian Ridge Regression, as
well as simple keyword mappings, all with promising
results~\cite{LarkeyCroft95,RibeiroNeto2001,PakhomovEtAl06,Ruch2008,Lita2008}.

Our dataset was gathered from the clinical data warehouse of
NewYork-Presbyterian Hospital. It consists of 6,000 discharge summaries and
their associated ICD-9 codes (7,298 distinct codes overall), representing all
the discharges from the hospital in 2009. Summaries have 8.39 associated ICD-9
codes on average (std dev=5.01) and contain an average of 536.57 terms after
preprocessing (std dev=300.29). We split our dataset into 5,000 discharge
summaries for training and 1,000 for testing.

The text of the discharge summaries was tokenized with
NLTK.\footnote{http://www.nltk.org} Vocabulary was determined as the top 10,000
tokens with highest document frequency (exclusive of names,
places and other identifying numbers). Each discharge summary is thus
represented as counts over the 10,000-word vocabulary. The study was approved
by the Institutional Review Board and follows HIPAA (Health
Insurance Portability and Accountability Act) privacy guidelines.

%For each hospitalization there are usually several ICD-9 codes assigned
%for billing purposes. These codes are known to be quite specific but
%not very sensitive \citep{}. Regardless of that fact,
%this is one of the only sources for information on patient diagnoses
%aside from the free text. %Aside from prediction, one of the goals is to compare the sensitivity of predictions from the HSLDA model in comparison to the codes in a case where a test closer to ground truth is available. For this we will compare whether predictions for the ICD-9 code associated with anemia are better predicted by HSLDA or by the ICD-9 codes. Anemia was chosen because hemoglobin values are readily available and the definition of anemia according the World Health Organization is approximately 12.5, with a threshold of 12 for women and 13 for men [citation].

\subsubsection{Product Category Prediction}

In this experiment, we look at product descriptions and their
categorizations according to a product hierarchy. Product ID's
and categorizations were obtained from the Stanford Network Analysis Platform
(SNAP) dataset~\citep{SNAP}. Product descriptions were crawled from the
amazon.com website directly. We were able to deduce the structure of the
hierarchy for the Amazon.com products directly since all ancestors in the
hierarchy were included with each category label. For example, {}``DVD / Genres
/ Science Fiction \& Fantasy / Classic Sci-Fi'' is a single product category 
for the DVD, {}``The Time Machine.''

Our dataset contains 15,130 product descriptions for training and 1,000
for testing. The product descriptions are shorter than the discharge summaries
(91.89 terms on average, std dev=53.08). Overall, there are 2,691 unique codes.
Products are assigned on average 9.01 codes (std dev=4.91). The vocabulary
consists of the most frequent 30,000 words omitting stopwords. 

\subsection{Comparison Models}

We applied HSLDA, along with three other closely related models, to
the clinical data and the retail product data. Specifically, we evaluate
models including sLDA with independent regressors (hierarchical constraints
on labels ignored), HSLDA fit by performing LDA followed by tree-conditional
regressions, and HSLDA fit with fixed random regression parameters.
We will refer to the three comparison models as the sLDA model, the
separate HSLDA model, and the random HSLDA model, respectively. These
models were chosen as to highlight performance in absence of hierarchical
constraints, the effect of the combined inference procedure, and regression
performance attributable solely to the hierarchical constraints.

We evaluated model performance for all three models with a range of
values for $\mu$ ($\mu\in\left\{ -3,-2.8,-2.6,\ldots,1\right\} )$,
the mean prior parameter for regression parameters.


\subsection{Evaluation}

%\begin{itemize}
%\item Talk about hierarchy and no hierarchy
%\item Make explicit calculation of sensitivity and 1-specificity
%\end{itemize}
For each dataset, a held out set of 1,000 documents and accompanying
labels were used for evaluation. The two main methods of evaluation
for the model are prediction and topic quality. To evaluate predictive
performance for all comparison models equivalently, each model was
evaluated with two methods. The first evaluation method augments the
observed labels in the held out set with their ancestors and considers
all other non-existant labels to be negative. The second method ignores
the ancestors of the observed labels in the held out set and considers
all other non-existant labels to be negative. This uniform treatment
of ancestors allows for a fair comparison of the models.

The two measures for predictive performance used here include the
true positive rate and the false positive rate evaluated based on
$p\left(y_{l,\hat{d}}\mid w_{1:N,d}\right)$ for each label in each
model.
