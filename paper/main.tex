\documentclass{article}
\usepackage{nips10submit_e,times}
%\documentstyle[nips07submit_09,times]{article}
\usepackage[square,numbers]{natbib}
\usepackage{amsmath, epsfig}
\usepackage{amsfonts}
\usepackage{subfigure}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{easybmat}
\usepackage{footmisc}
\usepackage{lscape}
\renewcommand\algorithmiccomment[1]{// \textit{#1}}
%
\newcommand{\ignore}[1]{}
\newcommand{\comment}[1]{}
\DeclareMathOperator*{\argmax}{arg\,max}

\title{Hierarchically Supervised Latent Dirichlet Allocation}
%Supervised Topic Modeling in Clinical Text}

\author{
Adler Perotte\hspace{1cm} Nicholas Bartlett \hspace{1cm} Noemie Elhadad \hspace{1cm} Frank Wood\\
Columbia University, New York, NY 10027, USA \\
\texttt{\{ajp9009@dbmi,bartlett@stat,noemie@dbmi,fwood@stat\}.columbia.edu}
%\texttt{pfau@neurotheory.columbia.edu} 
%\texttt{\{bartlett,fwood\}@stat.columbia.edu} 
}



% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\X}{\mathcal{X}}


\nipsfinalcopy

\begin{document}

\maketitle

\begin{abstract}
\input{abstract}
\end{abstract}

\section{Introduction}
\label{sec:introduction}
\input{introduction}


%\section{Background}




\section{Model}
\label{sec:model}
\input{model}

\section{Inference}
\label{sec:inference}
\input{inference}

\section{Related Work}

\label{sec:related_work} 
\input{related_work}

\section{Experiments}

\label{sec:experiments}

%\begin{itemize}
%\item Describe Data for both Experiments
%\item Describe the Four different conditions
%\item Describe how they are all evaluated, with and without hierarchy
%\end{itemize}
In the section we describe the application of HSLDA for prediction
in two hierarchically structured domains. Firstly, we describe using
discharge summaries to predict diagnoses, encoded as ICD-9 codes.
Discharge summaries are documents that are authored by clinicians
to summarize the course of a hospitalization. ICD-9 codes are used
mainly for billing purposes to indicate the conditions for which a
patient was treated. Secondly, we describe using Amazon.com product
descriptions to predict product categories.


\subsection{Data and Pre-Processing}


\subsubsection{Diagnosis Prediction}

Our data set was gathered from the clinical data warehouse of NewYork-Presbyterian
Hospital. The data consisted of free-text discharge summaries and
their respective ICD-9 codes. A discharge summary is a clinical report
prepared by a physician or other health professional at the conclusion
of a hospital stay or series of treatments. The note outlines the
patient's chief complaint, diagnostic findings, therapy administered,
patient's response to the chosen therapy, the treatment plan and the
recommendations upon discharge. The ICD-9 codes used to structure
the discharge summary data are part of a controlled terminology which
is the international standard diagnostic classification for epidemiological,
health management, and clinical purposes. The ICD-9 codes are organized
in a rooted-tree structure, with each edge representing an is-a relationship
between parent and child, such that the parent diagnosis subsumes
the child diagnosis. For example, the code representing {}``Pneumonia
due to adenovirus'' is a child of the code representing {}``Viral
pneumonia'' where the former is a type of the latter. In the hospital,
ICD-9 codes are generated manually by trained medical coders, who
review all the information in the discharge summary.

The text of the discharge summaries were pre-processed such that each
document would be represented as counts over a 10,000 word vocabulary.
The Natural Language Toolkit was used to tokenize the text. A vocabulary
was identified by first sorting terms based on a global term frequency-inverse
document frequency measure. The top 10,000 words which were not identifying
in some way (a name, place, or identifying number) were selected for
inclusion in the vocabulary.

For each hospitalization there are usually several ICD-9 codes assigned
for billing purposes. These codes are known to be quite specific but
not very sensitive \citep{Birmetal2005}. Regardless of that fact,
this is one of the only sources for information on patient diagnoses
aside from the free text. %Aside from prediction, one of the goals is to compare the sensitivity of predictions from the HSLDA model in comparison to the codes in a case where a test closer to ground truth is available. For this we will compare whether predictions for the ICD-9 code associated with anemia are better predicted by HSLDA or by the ICD-9 codes. Anemia was chosen because hemoglobin values are readily available and the definition of anemia according the World Health Organization is approximately 12.5, with a threshold of 12 for women and 13 for men [citation].


We worked within the guidelines of the Health Insurance Portability
and Accountability Act (HIPAA), which protects patient privacy and
the security of potentially identifying medical material, known as
personal health information (PHI). HIPAA covers any information within
a medical record that was created, used, or disclosed during the course
of providing a health care service and that can be used to identify
an individual. This study was approved by the Institutional Review
Board.


\subsubsection{Product Category Prediction}

Data for these experiments were obtained partially from the Stanford
Network Analysis Platform (SNAP) Amazon product metadata dataset \citep{SNAP}
and partially directly from the the Amazon.com website \citep{AMAZON}.
The product ID's and categorizations were obtained from the SNAP dataset
and the product descriptions were obtained directly from the website.

We were able to deduce the structure of the hierarchy for the Amazon.com
products directly since all ancestors in the hierarchy were included
with each category label. For example, {}``DVD / Genres / Science
Fiction \& Fantasy / Classic Sci-Fi'' is a single product category
for the DVD, {}``The Time Machine''. Each product was labeled with
multiple categories.

The vocabulary for this experiment was created by including the most
frequent 30K words omitting stopwords.


\subsection{Comparison Models}

We applied HSLDA, along with three other closely related models, to
the clinical data and the retail product data. Specifically, we evaluate
models including sLDA with independent regressors (hierarchical constraints
on labels ignored), HSLDA fit by performing LDA followed by tree-conditional
regressions, and HSLDA fit with fixed random regression parameters.
We will refer to the three comparison models as the sLDA model, the
separate HSLDA model, and the random HSLDA model, respectively. These
models were chosen as to highlight performance in absence of hierarchical
constraints, the effect of the combined inference procedure, and regression
performance attributable solely to the hierarchical constraints.

We evaluated model performance for all three models with a range of
values for $\mu$ ($\mu\in\left\{ -3,-2.8,-2.6,\ldots,1\right\} )$,
the mean prior parameter for regression parameters.


\subsection{Evaluation}

%\begin{itemize}
%\item Talk about hierarchy and no hierarchy
%\item Make explicit calculation of sensitivity and 1-specificity
%\end{itemize}
For each dataset, a held out set of 1,000 documents and accompanying
labels were used for evaluation. The two main methods of evaluation
for the model are prediction and topic quality. To evaluate predictive
performance for all comparison models equivalently, each model was
evaluated with two methods. The first evaluation method augments the
observed labels in the held out set with their ancestors and considers
all other non-existant labels to be negative. The second method ignores
the ancestors of the observed labels in the held out set and considers
all other non-existant labels to be negative. This uniform treatment
of ancestors allows for a fair comparison of the models.

The two measures for predictive performance used here include the
true positive rate and the false positive rate evaluated based on
$p\left(y_{l,\hat{d}}\mid w_{1:N,d}\right)$ for each label in each
model.

\section{Results}
\label{sec:results}

\begin{figure}[htbp]
\begin{center}
\subfigure[][\label{1a}]{\includegraphics[width=.49\textwidth]{figs/ROC_comparison_ancestors}}
\subfigure[][\label{1b}]{\includegraphics[width=.49\textwidth]{figs/ROC_comparison_leafs}}
\subfigure[][\label{1c}]{\includegraphics[width=.49\textwidth]{figs/sens_comparison_leafs}}
\subfigure[][\label{1d}]{\includegraphics[width=.49\textwidth]{figs/one_m_spec_comparison_leafs}}
\caption{Out-of-sample ICD-9 code prediction from patient free-text discharge records.  In all figures solid is HSLDA, dashed are independent regressors + sLDA (hierarchical constraints on labels ignored), dotted is HSLDA fit by running LDA first then running tree-conditional regressions, and dot-dashed is HSLDA fit with fixed random regression parameters.  Top row:  \subref{1a} includes ancestor prediction performance,  \subref{1b} results are for given (leaf) labels alone.  Bottom row: \subref{1c} are the sensitivity curves from \subref{1b} aligned on threshold value,  \subref{1d} are the 1-specificity curves from \subref{1b} aligned on threshold value.}
\label{fig:icd9}
\end{center}
\end{figure}

\begin{figure}[htbp]
\begin{center}
\subfigure[][\label{2a}]{\includegraphics[width=.49\textwidth]{figs/ROC_comparison_ancestors_amazon}}
\subfigure[][\label{2b}]{\includegraphics[width=.49\textwidth]{figs/ROC_comparison_leafs_amazon}}
\subfigure[][\label{2c}]{\includegraphics[width=.49\textwidth]{figs/sens_comparison_leafs_amazon}}
\subfigure[][\label{2d}]{\includegraphics[width=.49\textwidth]{figs/one_m_spec_comparison_leafs}}
\caption{Out-of-sample Amazon product code predictions from product free-text descriptions.  In all figures solid is HSLDA, dashed are independent regressors + sLDA (hierarchical constraints on labels ignored), dotted is HSLDA fit by running LDA first then running tree-conditional regressions, and dot-dashed is HSLDA fit with fixed random regression parameters.  Top row:  \subref{2a} includes ancestor prediction performance,  \subref{2b} results are for given (leaf) labels alone.  Bottom row: \subref{2c} are the sensitivity curves from \subref{2b} aligned on threshold value,  \subref{2d} are the 1-specificity curves from \subref{2b} aligned on threshold value.}
\label{fig:icd9}
\end{center}
\end{figure}

%We ran the Gibbs sampler for two days on 500 documents with 20 topics to learn the sLDA model which we then followed by a prediction of ICD-9 codes for 100 documents. The prediction resulted in 38 documents with at least 1 ICD-9 code assignment while some had over 200 assignments.


%\begin{figure}[h]%tbp] %  figure placement: here, top, bottom, or page
%   \centering
%   \includegraphics[width=4in]{number_of_codes_histogram} 
%   \caption{distribution of ICD-9 code assignments over all of the 100 documents predicted}
%   \label{fig:example}
%\end{figure}


%The accuracy of the prediction was tested against the actual ICD-9 codes in the EHR record for each discharge summary.  Different statistical measures were applied to asses the predictions.


%\begin{figure}[h]%tbp] %  figure placement: here, top, bottom, or page
%   \centering
%   \includegraphics[width=4in]{signal_detection} 
%   \caption{sensitivity, specificity, precision, recall and F-measure.}
%   \label{fig:example}
%\end{figure}


%Although the sensitivty is not as high as we would have hoped, it is important to note that the sLDA was run on a very limited sample of documents (500) due to time constraints.  Additionally, the “gold standard” data used are actual hospital results which are very noisy given the incompleteness and introduction of human error.



\section{Discussion}
\label{sec:discussion}
\input{discussion}
%We have proposed a new method for prediction of ICD-9 codes given a medical discharge summary note.  Our method explicitly takes advantage of the ICD-9 hierarchical IS-A relationships as well as the word distributions in each document.  We used Gibbs sampling for a supervised latent dirichlet allocation model to learn a distribution of words over twenty topics and their appropriate ICD-9 codes (response variable). The model was then tested on 100 discharge summaries for which the ICD-9 diagnoses were known.  The amount of ICD-9 codes produced for some discharge summaries was quite high because of the way the model worked with the IS-A hierarchy, highlighting the completeness of ICD-9 coding when using this sLDA model.  Overall, the model had extremely good specificity while the sensitivity was lower than anticipated.  Given our limited set of documents, and very noisy gold standard it would not be accurate to discount the use of sLDA in medical document classification and automatic coding.
%In future work, we plan to significantly expand the set of documents used to train the model (up to 15,000), as well as run several different iterations to learn the optimal topic number.  


%A more automated approach for coding patient visits with the ICD-9 hierarchy has great promise to  reduce health care costs and increase completeness of medical coded data.  We hope that this implementation of supervised LDA will serve as preliminary work for a much larger study with more documents in the future.


\begin{small} \bibliographystyle{plainnat} \bibliographystyle{plainnat}
\bibliographystyle{plainnat}
\bibliography{refs}


\end{small} 

\end{document}

